{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67b596eb",
   "metadata": {},
   "source": [
    "# 02_model_training.ipynb\n",
    "\n",
    "This notebook builds and trains a CNN for CIFAR-10 following the project requirements (3+ conv layers, batch norm, dropout, pooling, global avg pooling, softmax output). It also includes advanced options: residual blocks, Mixup/CutMix, transfer learning hooks, learning rate scheduling, callbacks, and evaluation (classification report + confusion matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b1dd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and helper functions\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json, os\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "from tensorflow.keras import layers, models, regularizers, optimizers, callbacks, applications\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print('TensorFlow available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefe079c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data (expects previous notebook/script to have prepared split datasets)\n",
    "# If you don't have them saved, this will create the 70/15/15 split and normalize.\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "X_all = np.concatenate([X_train, X_test])\n",
    "y_all = np.concatenate([y_train, y_test])\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_all, y_all, test_size=0.30, random_state=42, stratify=y_all)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp)\n",
    "\n",
    "# Normalize\n",
    "X_train = X_train.astype('float32')/255.0\n",
    "X_val = X_val.astype('float32')/255.0\n",
    "X_test = X_test.astype('float32')/255.0\n",
    "\n",
    "# One-hot\n",
    "num_classes = 10\n",
    "y_train_cat = to_categorical(y_train, num_classes)\n",
    "y_val_cat = to_categorical(y_val, num_classes)\n",
    "y_test_cat = to_categorical(y_test, num_classes)\n",
    "\n",
    "print('Shapes:', X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb3209b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation generators\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.1,\n",
    "    brightness_range=[0.8,1.2]\n",
    ")\n",
    "train_datagen.fit(X_train)\n",
    "\n",
    "train_gen = train_datagen.flow(X_train, y_train_cat, batch_size=64)\n",
    "val_gen = ImageDataGenerator().flow(X_val, y_val_cat, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe220213",
   "metadata": {},
   "source": [
    "## Model building helpers\n",
    "Includes a simple residual block and a base CNN builder with 3+ conv layers, batch norm, dropout, pooling, and global average pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969d7161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(x, filters, kernel_size=3, stride=1):\n",
    "    shortcut = x\n",
    "    x = layers.Conv2D(filters, kernel_size, padding='same', strides=stride, kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(filters, kernel_size, padding='same', strides=1, kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    # adjust shortcut if needed\n",
    "    if shortcut.shape[-1] != filters or stride != 1:\n",
    "        shortcut = layers.Conv2D(filters, 1, strides=stride, padding='same')(shortcut)\n",
    "        shortcut = layers.BatchNormalization()(shortcut)\n",
    "    x = layers.add([x, shortcut])\n",
    "    x = layers.ReLU()(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def build_cnn(input_shape=(32,32,3), num_classes=10, use_residual=False, dropout_rate=0.3):\n",
    "    inp = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(32, 3, padding='same', kernel_regularizer=regularizers.l2(1e-4))(inp)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(32, 3, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = layers.Conv2D(64, 3, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(64, 3, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    if use_residual:\n",
    "        x = residual_block(x, 128, stride=1)\n",
    "        x = residual_block(x, 128, stride=1)\n",
    "    else:\n",
    "        x = layers.Conv2D(128, 3, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.ReLU()(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.Dense(256)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    out = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = models.Model(inputs=inp, outputs=out)\n",
    "    return model\n",
    "\n",
    "# Build model\n",
    "model = build_cnn(use_residual=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bce33f",
   "metadata": {},
   "source": [
    "## Advanced: Transfer Learning (optional)\n",
    "You can uncomment and use one of these backbones (VGG16, ResNet50, MobileNetV2, EfficientNetB0) for transfer learning. Make sure to set `include_top=False` and `weights='imagenet'` and then add a GlobalAveragePooling + Dense head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58b4174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example transfer learning backbone (commented by default)\n",
    "# backbone = applications.ResNet50(include_top=False, weights='imagenet', input_shape=(32,32,3))\n",
    "# x = backbone.output\n",
    "# x = layers.GlobalAveragePooling2D()(x)\n",
    "# x = layers.Dense(256, activation='relu')(x)\n",
    "# out = layers.Dense(num_classes, activation='softmax')(x)\n",
    "# tl_model = models.Model(inputs=backbone.input, outputs=out)\n",
    "# for layer in backbone.layers:\n",
    "#     layer.trainable = False\n",
    "# tl_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# tl_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72734070",
   "metadata": {},
   "source": [
    "## Mixup and CutMix implementations (utilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a288570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixup utility\n",
    "def mixup(batch_x, batch_y, alpha=0.2):\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    batch_size = batch_x.shape[0]\n",
    "    index = np.random.permutation(batch_size)\n",
    "    mixed_x = lam * batch_x + (1 - lam) * batch_x[index]\n",
    "    mixed_y = lam * batch_y + (1 - lam) * batch_y[index]\n",
    "    return mixed_x, mixed_y\n",
    "\n",
    "# CutMix utility\n",
    "def cutmix(batch_x, batch_y, alpha=1.0):\n",
    "    batch_size, H, W, _ = batch_x.shape\n",
    "    index = np.random.permutation(batch_size)\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    rx = np.random.randint(W)\n",
    "    ry = np.random.randint(H)\n",
    "    rw = int(W * np.sqrt(1 - lam))\n",
    "    rh = int(H * np.sqrt(1 - lam))\n",
    "    x1 = np.clip(rx - rw // 2, 0, W)\n",
    "    y1 = np.clip(ry - rh // 2, 0, H)\n",
    "    x2 = np.clip(rx + rw // 2, 0, W)\n",
    "    y2 = np.clip(ry + rh // 2, 0, H)\n",
    "    new_x = batch_x.copy()\n",
    "    new_x[:, y1:y2, x1:x2, :] = batch_x[index, y1:y2, x1:x2, :]\n",
    "    lam_adjusted = 1 - ((x2 - x1) * (y2 - y1) / (W * H))\n",
    "    new_y = batch_y * lam_adjusted + batch_y[index] * (1 - lam_adjusted)\n",
    "    return new_x, new_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2cab1b",
   "metadata": {},
   "source": [
    "## Training setup: optimizer, callbacks, compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2b29e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "opt = optimizers.Adam(learning_rate=1e-3)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "checkpoint_cb = callbacks.ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_accuracy', mode='max')\n",
    "earlystop_cb = callbacks.EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
    "\n",
    "# Learning rate scheduler example (optional)\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch>0 and epoch%30==0:\n",
    "        return lr*0.5\n",
    "    return lr\n",
    "lr_cb = callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "cb_list = [checkpoint_cb, earlystop_cb, reduce_lr, lr_cb]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b4825f",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "Choose `use_mixup` or `use_cutmix` to apply those augmentations on-the-fly. Training is performed with generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151db9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "epochs = 100\n",
    "steps_per_epoch = len(train_gen)\n",
    "use_mixup = False\n",
    "use_cutmix = False\n",
    "\n",
    "# Custom training loop with mixup/cutmix support\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_with_augmentation(model, train_gen, val_gen, epochs, cb_list, use_mixup=False, use_cutmix=False):\n",
    "    history = { 'loss': [], 'accuracy': [], 'val_loss': [], 'val_accuracy': [] }\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        # train\n",
    "        batch_metrics = []\n",
    "        prog = tqdm(range(steps_per_epoch))\n",
    "        for i in prog:\n",
    "            X_batch, y_batch = next(train_gen)\n",
    "            if use_mixup:\n",
    "                X_batch, y_batch = mixup(X_batch, y_batch)\n",
    "            if use_cutmix:\n",
    "                X_batch, y_batch = cutmix(X_batch, y_batch)\n",
    "            res = model.train_on_batch(X_batch, y_batch)\n",
    "            prog.set_postfix({'loss':res[0], 'acc':res[1]})\n",
    "        # validate\n",
    "        val_res = model.evaluate(val_gen, verbose=0)\n",
    "        print('val loss, val acc ->', val_res)\n",
    "        # callbacks on_epoch_end simulation for ReduceLROnPlateau etc.\n",
    "        # NOTE: to fully use Keras callbacks, use model.fit with generators if not using mixup/cutmix.\n",
    "        history['val_loss'].append(val_res[0])\n",
    "        history['val_accuracy'].append(val_res[1])\n",
    "    return history\n",
    "\n",
    "# If you aren't using mixup/cutmix, you can simply call model.fit:\n",
    "if not (use_mixup or use_cutmix):\n",
    "    history = model.fit(train_gen, validation_data=val_gen, epochs=50, callbacks=cb_list)\n",
    "else:\n",
    "    history = train_with_augmentation(model, train_gen, val_gen, epochs=50, cb_list=cb_list, use_mixup=use_mixup, use_cutmix=use_cutmix)\n",
    "\n",
    "# Save history to JSON\n",
    "hist = history.history if hasattr(history, 'history') else history\n",
    "with open('training_history.json', 'w') as f:\n",
    "    json.dump(hist, f)\n",
    "\n",
    "print('Training complete or skipped (run cells to train).')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea5ea94",
   "metadata": {},
   "source": [
    "## Evaluation on test set and visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627d9b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model if exists, else use current model\n",
    "from tensorflow.keras.models import load_model\n",
    "model_path = 'best_model.h5'\n",
    "if os.path.exists(model_path):\n",
    "    print('Loading best_model.h5')\n",
    "    model = load_model(model_path)\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_acc = model.evaluate(ImageDataGenerator().flow(X_test, y_test_cat, batch_size=64), verbose=1)\n",
    "print('Test loss:', test_loss, 'Test acc:', test_acc)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(ImageDataGenerator().flow(X_test, batch_size=64, shuffle=False))\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "y_true = y_test.reshape(-1)\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_true, y_pred_labels))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred_labels)\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Plot training curves if history exists\n",
    "if os.path.exists('training_history.json'):\n",
    "    with open('training_history.json','r') as f:\n",
    "        hist = json.load(f)\n",
    "    if 'loss' in hist:\n",
    "        plt.figure(); plt.plot(hist.get('loss', []), label='train_loss'); plt.plot(hist.get('val_loss', []), label='val_loss'); plt.legend(); plt.title('Loss'); plt.show()\n",
    "    if 'accuracy' in hist:\n",
    "        plt.figure(); plt.plot(hist.get('accuracy', []), label='train_acc'); plt.plot(hist.get('val_accuracy', []), label='val_acc'); plt.legend(); plt.title('Accuracy'); plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
